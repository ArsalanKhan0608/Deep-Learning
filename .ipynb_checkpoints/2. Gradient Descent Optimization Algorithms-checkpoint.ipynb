{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b6353fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.105364304612469, Validation Loss: 1.0506513929506545\n",
      "Epoch 10, Loss: 0.13816787720294677, Validation Loss: 0.044747994374113725\n",
      "Epoch 20, Loss: 0.057654666599093664, Validation Loss: 0.022059327056193265\n",
      "Epoch 30, Loss: 0.05058895655218387, Validation Loss: 0.02288998463529905\n",
      "Epoch 40, Loss: 0.04833563733898785, Validation Loss: 0.009776008025516628\n",
      "Epoch 50, Loss: 0.04744727936003065, Validation Loss: 0.013258128231685337\n",
      "Epoch 60, Loss: 0.0470880181821431, Validation Loss: 0.014001155969004418\n",
      "Epoch 70, Loss: 0.04677522108942112, Validation Loss: 0.012438450292568428\n",
      "Epoch 80, Loss: 0.0466565855657776, Validation Loss: 0.015292039227321046\n",
      "Epoch 90, Loss: 0.04660256431345639, Validation Loss: 0.014676075558936397\n",
      "Epoch 100, Loss: 0.046587029974382674, Validation Loss: 0.01504352983200253\n",
      "Epoch 110, Loss: 0.046582650519847044, Validation Loss: 0.015288605755247245\n",
      "Epoch 120, Loss: 0.046581456208505, Validation Loss: 0.015298583893257828\n",
      "Epoch 130, Loss: 0.04658101122260647, Validation Loss: 0.015355380528219996\n",
      "Epoch 140, Loss: 0.046580773821099014, Validation Loss: 0.015312120398418528\n",
      "Epoch 150, Loss: 0.04658070019199782, Validation Loss: 0.015343720951707081\n",
      "Epoch 160, Loss: 0.04658065875547382, Validation Loss: 0.015328142388420712\n",
      "Epoch 170, Loss: 0.046580635924229445, Validation Loss: 0.015318391394917696\n",
      "Epoch 180, Loss: 0.04658061813642538, Validation Loss: 0.015322442844653113\n",
      "Epoch 190, Loss: 0.046580603744646606, Validation Loss: 0.01531479904286672\n",
      "Validation accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the softmax function\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "# SGD with Momentum Optimizer\n",
    "class SGDWithMomentum:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.v[key] = self.momentum * self.v[key] - self.learning_rate * grads[key]\n",
    "            params[key] += self.v[key]\n",
    "\n",
    "# Adam Optimizer\n",
    "class Adam:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.iteration = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "        \n",
    "        self.iteration += 1\n",
    "        for key in params.keys():\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.iteration)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.iteration)\n",
    "            params[key] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "            \n",
    "            \n",
    "class Adagrad:\n",
    "    def __init__(self, learning_rate=0.01, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.G = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.G is None:\n",
    "            self.G = {}\n",
    "            for key, val in params.items():\n",
    "                self.G[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.G[key] += grads[key] ** 2\n",
    "            params[key] -= self.learning_rate * grads[key] / (np.sqrt(self.G[key]) + self.epsilon)\n",
    "\n",
    "            \n",
    "class RMSprop:\n",
    "    def __init__(self, learning_rate=0.001, rho=0.9, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "        self.Eg = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.Eg is None:\n",
    "            self.Eg = {}\n",
    "            for key, val in params.items():\n",
    "                self.Eg[key] = np.zeros_like(val)\n",
    "        \n",
    "        for key in params.keys():\n",
    "            self.Eg[key] = self.rho * self.Eg[key] + (1 - self.rho) * (grads[key] ** 2)\n",
    "            params[key] -= self.learning_rate * grads[key] / (np.sqrt(self.Eg[key]) + self.epsilon)\n",
    "\n",
    "            \n",
    "\n",
    "class AdaMax:\n",
    "    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # First moment vector\n",
    "        self.u = None  # Infinity norm\n",
    "        self.iteration = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None or self.u is None:\n",
    "            self.m, self.u = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.u[key] = np.zeros_like(val)\n",
    "\n",
    "        self.iteration += 1\n",
    "        alpha_t = self.learning_rate / (1 - self.beta1 ** self.iteration)  # Corrects bias for first moment\n",
    "\n",
    "        for key in params.keys():\n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            # Update the infinity norm\n",
    "            self.u[key] = np.maximum(self.beta2 * self.u[key], np.abs(grads[key]))\n",
    "            # Update parameters\n",
    "            params[key] -= alpha_t * self.m[key] / (self.u[key] + self.epsilon)\n",
    "\n",
    "\n",
    "class FTRL:\n",
    "    def __init__(self, learning_rate=0.01, beta=1.0, l1=0.1, l2=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta = beta\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.z = None\n",
    "        self.n = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.z is None or self.n is None:\n",
    "            self.z, self.n = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.z[key] = np.zeros_like(val)\n",
    "                self.n[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.n[key] += grads[key] ** 2\n",
    "            sigma = (np.sqrt(self.n[key]) - np.sqrt(self.n[key] - grads[key] ** 2)) / self.learning_rate\n",
    "            self.z[key] += grads[key] - sigma * params[key]\n",
    "            params[key] = - ((self.z[key] - np.sign(self.z[key]) * self.l1) / \n",
    "                            ((self.beta + np.sqrt(self.n[key])) / self.learning_rate + self.l2))\n",
    "            params[key] *= np.abs(self.z[key]) > self.l1\n",
    "\n",
    "class NesterovAcceleratedGradient:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = {}\n",
    "            for key, val in params.items():\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            v_prev = self.v[key]\n",
    "            self.v[key] = self.momentum * self.v[key] - self.learning_rate * grads[key]\n",
    "            params[key] += -self.momentum * v_prev + (1 + self.momentum) * self.v[key]\n",
    "\n",
    "class AdaDelta:\n",
    "    def __init__(self, rho=0.95, epsilon=1e-6):\n",
    "        self.rho = rho\n",
    "        self.epsilon = epsilon\n",
    "        self.Eg = None\n",
    "        self.Edelta = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.Eg is None or self.Edelta is None:\n",
    "            self.Eg, self.Edelta = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.Eg[key] = np.zeros_like(val)\n",
    "                self.Edelta[key] = np.zeros_like(val)\n",
    "\n",
    "        for key in params.keys():\n",
    "            self.Eg[key] = self.rho * self.Eg[key] + (1 - self.rho) * (grads[key] ** 2)\n",
    "            delta = - (np.sqrt(self.Edelta[key] + self.epsilon) / np.sqrt(self.Eg[key] + self.epsilon)) * grads[key]\n",
    "            self.Edelta[key] = self.rho * self.Edelta[key] + (1 - self.rho) * (delta ** 2)\n",
    "            params[key] += delta\n",
    "            \n",
    "class Nadam:\n",
    "    def __init__(self, learning_rate=0.002, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.iteration = 0\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.m is None or self.v is None:\n",
    "            self.m, self.v = {}, {}\n",
    "            for key, val in params.items():\n",
    "                self.m[key] = np.zeros_like(val)\n",
    "                self.v[key] = np.zeros_like(val)\n",
    "\n",
    "        self.iteration += 1\n",
    "        for key in params.keys():\n",
    "            # Moving average of the gradients.\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * grads[key]\n",
    "            # Moving average of the squared gradients.\n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (grads[key] ** 2)\n",
    "\n",
    "            # Compute bias-corrected first moment estimate.\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.iteration)\n",
    "            # Compute bias-corrected second raw moment estimate.\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.iteration)\n",
    "\n",
    "            # Compute the Nesterov-accelerated gradient.\n",
    "            m_bar = (1 - self.beta1) * grads[key] + self.beta1 * m_hat\n",
    "\n",
    "            # Update parameters.\n",
    "            params[key] -= self.learning_rate * m_bar / (np.sqrt(v_hat) + self.epsilon)\n",
    "\n",
    "            \n",
    "# Neural Network\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.params = {\n",
    "            'W1': np.random.randn(input_size, hidden_size) * 0.1,\n",
    "            'b1': np.zeros(hidden_size),\n",
    "            'W2': np.random.randn(hidden_size, output_size) * 0.1,\n",
    "            'b2': np.zeros(output_size),\n",
    "        }\n",
    "\n",
    "    def forward(self, X):\n",
    "        z1 = np.dot(X, self.params['W1']) + self.params['b1']\n",
    "        a1 = np.maximum(0, z1)  # ReLU activation\n",
    "        z2 = np.dot(a1, self.params['W2']) + self.params['b2']\n",
    "        a2 = softmax(z2)  # Softmax activation\n",
    "        return a1, a2\n",
    "\n",
    "    def backward(self, X, a1, a2, y):\n",
    "        dz2 = a2 - y\n",
    "        dW2 = np.dot(a1.T, dz2)\n",
    "        db2 = np.sum(dz2, axis=0)\n",
    "\n",
    "        da1 = np.dot(dz2, self.params['W2'].T)\n",
    "        dz1 = da1\n",
    "        dz1[a1 <= 0] = 0  # Derivative of ReLU\n",
    "        dW1 = np.dot(X.T, dz1)\n",
    "        db1 = np.sum(dz1, axis=0)\n",
    "\n",
    "        grads = {'W1': dW1, 'b1': db1, 'W2': dW2, 'b2': db2}\n",
    "        return grads\n",
    "\n",
    "    def compute_loss(self, y_hat, y):\n",
    "        # Using a small epsilon to avoid log(0)\n",
    "        epsilon = 1e-15\n",
    "        return -np.mean(np.log(y_hat[np.arange(len(y_hat)), y.argmax(axis=1)] + epsilon))\n",
    "\n",
    "    def predict(self, X):\n",
    "        _, y_hat = self.forward(X)\n",
    "        return y_hat.argmax(axis=1)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, epochs, optimizer):\n",
    "        for epoch in range(epochs):\n",
    "            # Forward pass\n",
    "            a1, a2 = self.forward(X_train)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(a2, y_train)\n",
    "\n",
    "            # Backward pass\n",
    "            grads = self.backward(X_train, a1, a2, y_train)\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.update(self.params, grads)\n",
    "\n",
    "            # Validation loss\n",
    "            _, a2_val = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(a2_val, y_val)\n",
    "\n",
    "            # Print out the losses periodically\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch {epoch}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# One-hot encode the target variable\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Updated parameter\n",
    "y_onehot = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "\n",
    "# Initialize the neural network with the size of the input layer (number of features),\n",
    "# the size of the hidden layer, and the size of the output layer (number of classes)\n",
    "nn = NeuralNetwork(input_size=X_train_scaled.shape[1], hidden_size=10, output_size=y_train.shape[1])\n",
    "\n",
    "# Select an optimizer\n",
    "optimizer = SGDWithMomentum(learning_rate=0.01, momentum=0.9)\n",
    "# optimizer = Adam(learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8)\n",
    "\n",
    "# Train the neural network\n",
    "epochs = 200\n",
    "nn.train(X_train_scaled, y_train, X_val_scaled, y_val, epochs, optimizer)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_pred = nn.predict(X_val_scaled)\n",
    "\n",
    "# One-hot encoding is reversed to compare with the original labels\n",
    "y_val_labels = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val_labels, y_pred)\n",
    "print(f'Validation accuracy: {accuracy:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b3a836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
